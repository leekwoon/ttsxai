{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/nas/users/dahye/kw/tts/github_download/IMS-Toucan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from TrainingInterfaces.Text_to_Spectrogram.ToucanTTS.ToucanTTS import ToucanTTS\n",
    "from TrainingInterfaces.Text_to_Spectrogram.ToucanTTS.toucantts_train_loop_arbiter import train_loop\n",
    "from Utility.corpus_preparation import prepare_fastspeech_corpus\n",
    "from Utility.path_to_transcript_dicts import *\n",
    "from Utility.storage_config import MODELS_DIR\n",
    "from Utility.storage_config import PREPROCESSING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing\n",
    "\n",
    "from TrainingInterfaces.Text_to_Spectrogram.AutoAligner.AlignerDataset import AlignerDataset\n",
    "from TrainingInterfaces.Text_to_Spectrogram.AutoAligner.autoaligner_train_loop import train_loop as train_aligner\n",
    "from TrainingInterfaces.Text_to_Spectrogram.FastSpeech2.FastSpeechDataset import FastSpeechDataset\n",
    "from Utility.path_to_transcript_dicts import *\n",
    "from Utility.storage_config import MODELS_DIR\n",
    "\n",
    "\n",
    "def prepare_aligner_corpus(transcript_dict, corpus_dir, lang, device):\n",
    "    return AlignerDataset(transcript_dict, cache_dir=corpus_dir, lang=lang, loading_processes=os.cpu_count() if os.cpu_count() is not None else 30,\n",
    "                          cut_silences=True,\n",
    "                          device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from numpy import trim_zeros\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from torch.multiprocessing import Manager\n",
    "from torch.multiprocessing import Process\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Preprocessing.AudioPreprocessor import AudioPreprocessor\n",
    "from Preprocessing.TextFrontend import ArticulatoryCombinedTextFrontend\n",
    "from Utility.storage_config import MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc89413fe50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(131714)\n",
    "random.seed(131714)\n",
    "torch.random.manual_seed(131714)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared a FastSpeech dataset with 3181 datapoints in Corpora/Jim.\n"
     ]
    }
   ],
   "source": [
    "all_train_sets = list()  # YOU CAN HAVE MULTIPLE LANGUAGES, OR JUST ONE. JUST MAKE ONE ConcatDataset PER LANGUAGE AND ADD IT TO THE LIST.\n",
    "\n",
    "english_datasets = list()\n",
    "english_datasets.append(prepare_fastspeech_corpus(\n",
    "    transcript_dict=build_path_to_transcript_dict_generic_ljspeech(\"/nas/users/dahye/kw/tts/github_download/CrewChiefV4/CrewChiefV4/sounds/\"),\n",
    "    corpus_dir=os.path.join(PREPROCESSING_DIR, \"Jim\"),\n",
    "    lang=\"en\"))\n",
    "\n",
    "all_train_sets.append(ConcatDataset(english_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dict = build_path_to_transcript_dict_generic_ljspeech(\"/nas/users/dahye/kw/tts/github_download/CrewChiefV4/CrewChiefV4/sounds/\")\n",
    "corpus_dir=os.path.join(PREPROCESSING_DIR, \"Jim\")\n",
    "cache_dir = corpus_dir\n",
    "lang=\"en\"\n",
    "ctc_selection=True  # heuristically removes some samples which might be problematic.\n",
    "# For small datasets it's best to turn this off and instead inspect the data with the scorer, if there are any issues.\n",
    "fine_tune_aligner=True\n",
    "use_reconstruction=True\n",
    "phone_input=False\n",
    "save_imgs=False\n",
    "\n",
    "# config for aligner dataset\n",
    "loading_processes=os.cpu_count() if os.cpu_count() is not None else 30\n",
    "min_len_in_seconds=1\n",
    "max_len_in_seconds=20\n",
    "cut_silences=True\n",
    "rebuild_cache=False\n",
    "verbose=False\n",
    "device=\"cpu\"\n",
    "phone_input=False\n",
    "allow_unknown_symbols=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared an Aligner dataset with 3351 datapoints in Corpora/Jim.\n"
     ]
    }
   ],
   "source": [
    "aligner_datapoints = AlignerDataset(transcript_dict, cache_dir=corpus_dir, lang=lang, phone_input=phone_input, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner_dir = os.path.join(corpus_dir, \"Aligner\")\n",
    "aligner_loc = os.path.join(corpus_dir, \"Aligner\", \"aligner.pt\")\n",
    "\n",
    "train_dataset=aligner_datapoints\n",
    "device=torch.device(\"cuda\")\n",
    "save_directory=aligner_dir\n",
    "steps=len(aligner_datapoints) * 2  # relatively good heuristic\n",
    "batch_size=32 if len(aligner_datapoints) > 32 else len(aligner_datapoints) // 2\n",
    "path_to_checkpoint=None\n",
    "fine_tune=False\n",
    "debug_img_path=aligner_dir\n",
    "resume=False\n",
    "use_reconstruction=use_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = ArticulatoryCombinedTextFrontend(language=lang)\n",
    "ap = AudioPreprocessor(input_sr=22050, output_sr=16000, melspec_buckets=80, hop_length=256, n_fft=1024,\n",
    "    cut_silence=cut_silences, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, transcript in transcript_dict.items():\n",
    "    if transcript.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        wave, sr = sf.read(path)\n",
    "    except:\n",
    "        print(f\"Problem with an audio file: {path}\")\n",
    "        continue\n",
    "\n",
    "    dur_in_seconds = len(wave) / sr\n",
    "    if not (min_len_in_seconds <= dur_in_seconds <= max_len_in_seconds):\n",
    "        if verbose:\n",
    "            print(f\"Excluding {path} because of its duration of {round(dur_in_seconds, 2)} seconds.\")\n",
    "        continue\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")  # otherwise we get tons of warnings about an RNN not being in contiguous chunks\n",
    "            norm_wave = ap.audio_to_wave_tensor(normalize=True, audio=wave)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    dur_in_seconds = len(norm_wave) / 16000\n",
    "    if not (min_len_in_seconds <= dur_in_seconds <= max_len_in_seconds):\n",
    "        if verbose:\n",
    "            print(f\"Excluding {path} because of its duration of {round(dur_in_seconds, 2)} seconds.\")\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim zeros!\n",
    "norm_wave = torch.tensor(trim_zeros(norm_wave.numpy()))\n",
    "cached_text = tf.string_to_tensor(transcript, handle_missing=False, input_phonemes=phone_input).squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_text_len = torch.LongTensor([len(cached_text)]).numpy()\n",
    "cached_speech = ap.audio_to_mel_spec_tensor(audio=norm_wave, normalize=False,\n",
    "                                            explicit_sampling_rate=16000).transpose(0, 1).cpu().numpy()\n",
    "cached_speech_len = torch.LongTensor([len(cached_speech)]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_vectors_to_id_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py:450\u001b[0m, in \u001b[0;36mArticulatoryCombinedTextFrontend.text_vectors_to_id_sequence\u001b[0;34m(self, text_vector)\u001b[0m\n\u001b[1;32m    <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=446'>447</a>\u001b[0m \u001b[39mfor\u001b[39;00m vector \u001b[39min\u001b[39;00m text_vector:\n\u001b[1;32m    <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=447'>448</a>\u001b[0m     \u001b[39mif\u001b[39;00m vector[get_feature_to_index_lookup()[\u001b[39m\"\u001b[39m\u001b[39mword-boundary\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=448'>449</a>\u001b[0m         \u001b[39m# we don't include word boundaries when performing alignment, since they are not always present in audio.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=449'>450</a>\u001b[0m         features \u001b[39m=\u001b[39m vector\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=450'>451</a>\u001b[0m         \u001b[39mif\u001b[39;00m vector[get_feature_to_index_lookup()[\u001b[39m\"\u001b[39m\u001b[39mvowel\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m vector[get_feature_to_index_lookup()[\u001b[39m\"\u001b[39m\u001b[39mnasal\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=451'>452</a>\u001b[0m             \u001b[39m# for the sake of alignment, we ignore the difference between nasalized vowels and regular vowels\u001b[39;00m\n\u001b[1;32m    <a href='file:///nas/users/dahye/kw/tts/github_download/IMS-Toucan/Preprocessing/TextFrontend.py?line=452'>453</a>\u001b[0m             features[get_feature_to_index_lookup()[\u001b[39m\"\u001b[39m\u001b[39mnasal\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "tf.text_vectors_to_id_sequence(text_vector=cached_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared an Aligner dataset with 3351 datapoints in Corpora/Jim.\n"
     ]
    }
   ],
   "source": [
    "aligner_datapoints = AlignerDataset(transcript_dict, cache_dir=corpus_dir, lang=lang, phone_input=phone_input, device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 45, 29, 34, 29, 24, 39, 25, 11, 41, 37, 25, 11, 41, 13, 34,  9, 11,\n",
       "        11, 34, 32, 21, 37, 21, 34, 24,  9, 11,  8, 18, 42, 32, 21, 37,  0,  9,\n",
       "        11, 41,  7, 32, 22, 29,  8, 18,  7, 22, 10, 34, 42, 16, 33, 55, 39, 31,\n",
       "         0,  1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligner_datapoints[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner_dir = os.path.join(corpus_dir, \"Aligner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(aligner_dir, \"aligner.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Models/'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(MODELS_DIR, \"Aligner\", \"aligner.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_speech_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlignerDataset(\n",
    "    transcript_dict, cache_dir=corpus_dir, lang=lang, \n",
    "    phone_input=phone_input, device=torch.device(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4b1798ef45001c5ce94e2c04e054842b0610fb81d557903a04e930ffb6c3821"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 ('ttsxai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
